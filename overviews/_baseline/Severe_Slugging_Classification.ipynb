{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e96c1a",
   "metadata": {},
   "source": [
    "# 3W - Strategy Thinking project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a69a3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T11:28:58.281245Z",
     "start_time": "2022-11-17T11:28:55.991830Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS AND CONFIGURATIONS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import FuncFormatter\n",
    "# import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# importing classes and functions for PCA, hyperparametrisation and cross-validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb8439",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Preprocessing a dataset through data characterisation involves summarising the features and characteristics present in the data using statistical measures and visualisations techniques such as bar charts and scatter plots. After this stage, it should be possible to identify biases, patterns, trends, and any missing or irrelevant data in the data set that may need to be addressed.\n",
    "\n",
    "This dataset is composed by instances of eight types of undesirable events characterized by eight process variables from three different sources: real instances, simulated instances and hand-drawn instances. All real instances were taken from the plant information system that is used to monitor the industrial processes at an operational unit in Brazilian state of Espírito Santo. The simulated instances were all generated using OLGA ([Schlumberger](https://www.software.slb.com/products/olga)), a dynamic multiphase flow simulator that is widely used by oil companies worldwide (Andreolli, 2016). Finally, the hand-drawn instances were generated by a specific tool developed by Petrobras researchers for this dataset to incorporate undesirable events classfied as rare. \n",
    " \n",
    "### Data Characterisation\n",
    "The data consists of over 50 million observations, with 13 columns of data for each observation. The first column, `label`, indicates the event type for each observation. The second column, `well`, contains the name of the well the observation was taken from. Hand-drawn and simulated instances have fixed names for in this column, while real instances have names masked with incremental id. The third column, `id`, is an identifier for the observation and it is incremental for hand-drawn and simulated instances, while each real instance has an id generated from its first timestamp. The columns representing the process variables are:\n",
    "\n",
    "* **P-PDG**: pressure variable at the Permanent Downhole Gauge (PDG) - installed on Christmas Tree;\n",
    "* **P-TPT**: pressure variable at the Temperature and Pressure Transducer (TPT) - installed on Christmas Tree;\n",
    "* **T-TPT**: temperature variable at the Temperature and Pressure Transducer (TPT);\n",
    "* **P-MON-CKP**: pressure variable upstream of the production choke (CKP) - located on platform;\n",
    "* **T-JUS-CKP**: temperature variable downstream of the production choke (CKP);\n",
    "* **P-JUS-CKGL**: pressure variable upstream of the gas lift choke (CKGL);\n",
    "* **T-JUS-CKGL**: temperature variable upstream of the gas lift choke (CKGL);\n",
    "* **QGL**: gas lift flow rate;\n",
    "\n",
    "The pressure features are measured in Pascal (Pa), the volumetric flow rate features are measured in standard cubic meters per second (SCM/s), and the temperature features are measured in degrees Celsius (°C).\n",
    "\n",
    "Other information are also loaded into each pandas Dataframe:\n",
    "\n",
    "* **label**: instance label (event type) - target variable;\n",
    "* **well**: well name. Hand-drawn and simulated instances have fixed names (respectively, `drawn` and `simulated`. Real instances have names masked with incremental id;\n",
    "* **id**: instance identifier. Hand-drawn and simulated instances have incremental id. Each real instance has an id generated from its first timestamp;\n",
    "* **class**: Although it can be used to identify periods of normal operation, fault transients, and faulty steady states, which can help with diagnosis and maintenance, it is a category which results from label, which is our target here\n",
    "\n",
    "The labels are:\n",
    "* 0 - Normal Operation = `Normal`\n",
    "* 1 - Abrupt Increase of BSW = `AbrIncrBSW`\n",
    "* 2 - Spurious Closure of DHSV = `SpurClosDHSW`\n",
    "* 3 - Severe Slugging = `SevSlug`\n",
    "* 4 - Flow Instability = `FlowInst`\n",
    "* 5 - Rapid Productivity Loss = `RProdLoss`\n",
    "* 6 - Quick Restriction in PCK\t= `QuiRestrPCK`\n",
    "* 7 - Scaling in PCK = `ScalingPCK`\n",
    "* 8 - Hydrate in Production Line = `HydrProdLine`\n",
    "\n",
    "More information about these variables can be obtained from the following publicly available documents:\n",
    "\n",
    "* ***Option in Portuguese***: R.E.V. Vargas. Base de dados e benchmarks para prognóstico de anomalias em sistemas de elevação de petróleo. Universidade Federal do Espírito Santo. Doctoral thesis. 2019. https://github.com/petrobras/3W/raw/master/docs/doctoral_thesis_ricardo_vargas.pdf.\n",
    "* ***Option in English***: B.G. Carvalho. Evaluating machine learning techniques for detection of flow instability events in offshore oil wells. Universidade Federal do Espírito Santo. Master's degree dissertation. 2021. https://github.com/petrobras/3W/raw/master/docs/master_degree_dissertation_bruno_carvalho.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c59f2",
   "metadata": {},
   "source": [
    "In order to maintain the realistic aspects of the data, the dataset was extracted without preprocessing, including the presence of `NaN` values, frozen variables due to sensor or communication issues, instances with varying sizes, and outliers (R.E.V. Vargas, et al. 2019). \n",
    "\n",
    "From all 50,822,124 entries, 3,086,851 are duplicated, that is, approximately 6.07% of total. These duplicated rows may be related to frozen variables from real instances, as simulated and hand-drawn instances are naturally free of such problems. Although no missing values were found for columns `label`, `well`, and `id`, other features presented null or absent values. Notably, the column `T-JUS-CKGL` turned out to be completely empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3057ff77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T11:28:58.882142Z",
     "start_time": "2022-11-17T11:28:58.286260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13952911, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('3Wdataset_real.csv', index_col=None, parse_dates=['timestamp'])\n",
    "\n",
    "# only using real, since simulated and drawn doesnt have QGL and P-JUS-CKGL\n",
    "# df_real.info()\n",
    "\n",
    "# df_drawn = pd.read_csv('3Wdataset_drawn.csv', index_col=None)\n",
    "# # df_drawn.info()\n",
    "\n",
    "# df_sim1 = pd.read_csv('3Wdataset_simulated_1of2.csv', index_col=None)\n",
    "# # df_sim1.info()\n",
    "\n",
    "# df_sim2 = pd.read_csv('3Wdataset_simulated_2of2.csv', index_col=None)\n",
    "# # df_sim2.info()\n",
    "\n",
    "# df = pd.concat([\n",
    "#     df_real,\n",
    "#     df_sim1,\n",
    "#     df_sim2,\n",
    "#     df_drawn\n",
    "# ])\n",
    "\n",
    "# df = df.drop('source', axis=1)\n",
    "\n",
    "# dismissing temporary DFs to release memory\n",
    "# del df_sim1\n",
    "# del df_sim2\n",
    "# del df_real\n",
    "# del df_drawn\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0565ba8",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d45927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d354b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b48c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import create_report\n",
    "report = create_report(df, title='My Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a802a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing values\n",
    "missing = df.isnull()\n",
    "missing.sum()\n",
    "\n",
    "total_missing = df.isnull().sum()\n",
    "percent_missing = total_missing * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'percent': percent_missing, 'total':total_missing})\n",
    "\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94665569",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing or null class column\n",
    "df_clean = df.dropna(subset=['P-PDG','P-TPT','T-JUS-CKP','P-MON-CKP','T-TPT','P-MON-CKP','QGL','P-JUS-CKGL'])\n",
    "\n",
    "# first interaction will dismiss timestamp in order to remove duplicates, even if it overlooks frozen\n",
    "# variables due to sensor or communication issues\n",
    "\n",
    "# removing class, since it results from label\n",
    "# removing ID from df\n",
    "\n",
    "df_clean = df_clean.drop([\n",
    "#     'timestamp', \n",
    "    'class',\n",
    "    'T-JUS-CKGL', # T-JUS-CKGL is empty\n",
    "    'id', \n",
    "    'source'\n",
    "], axis=1)\n",
    "\n",
    "# checking duplicated rows after removing ids\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing values\n",
    "missing = df_clean.isnull()\n",
    "missing.sum()\n",
    "\n",
    "total_missing = df_clean.isnull().sum()\n",
    "percent_missing = total_missing * 100 / len(df_clean)\n",
    "missing_value_df = pd.DataFrame({'percent': percent_missing, 'total':total_missing})\n",
    "\n",
    "# Total of Missing Values\n",
    "missing_value_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ef122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Scaling, with Outliers\n",
    "\n",
    "df_clean.plot(kind='box', figsize=(8, 5), showfliers=True)\n",
    "plt.title('Boxplot of Features Before Scaling')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Value')\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914b1c4",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86359751",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_feat = df_clean\n",
    "\n",
    "# Change 'label' column to object dtype\n",
    "dt_feat['label'] = dt_feat['label'].astype('object') \n",
    "\n",
    "# Create boolean columns for each label\n",
    "label_dummies = pd.get_dummies(dt_feat['label'], prefix='label')\n",
    "dt_feat = pd.concat([dt_feat, label_dummies], axis=1)\n",
    "\n",
    "# Rename boolean columns\n",
    "column_names = {\n",
    "    'label_0': 'Normal',\n",
    "    'label_1': 'AbrIncrBSW',\n",
    "    'label_2': 'SpurClosDHSW',\n",
    "    'label_3': 'SevSlug',\n",
    "    'label_4': 'FlowInst',\n",
    "    'label_5': 'RProdLoss',\n",
    "    'label_6': 'QuiRestrPCK',\n",
    "    'label_7': 'ScalingPCK',\n",
    "    'label_8': 'HydrProdLine'\n",
    "}\n",
    "dt_feat = dt_feat.rename(columns=column_names)\n",
    "\n",
    "# Drop the original 'label' column and Normal column, since all other events must be 0\n",
    "dt_feat = dt_feat.drop(['label','Normal'], axis=1)\n",
    "\n",
    "dt_feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_feat_target = dt_feat.drop([\n",
    "    'AbrIncrBSW',\n",
    "    'SpurClosDHSW',\n",
    "#     'SevSlug',\n",
    "    'FlowInst',\n",
    "    'RProdLoss',\n",
    "    'QuiRestrPCK',\n",
    "    'ScalingPCK'\n",
    "#     ,'HydrProdLine'\n",
    "], axis=1)\n",
    "\n",
    "dt_feat_target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlations\n",
    "corr = dt_feat_target.corr()\n",
    "\n",
    "# Creating a boolean mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Plotting the heatmap with the mask\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.set(font_scale=0.6)\n",
    "sns.heatmap(corr, annot=True, mask=mask, ax=ax, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c517a13",
   "metadata": {},
   "source": [
    "### Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining features (X) and label (y)\n",
    "\n",
    "target = 'SevSlug'\n",
    "\n",
    "X = dt_feat_target.drop([target,'timestamp','well'], axis=1)\n",
    "y = dt_feat_target['SevSlug']\n",
    "\n",
    "# splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# calculating the percentage of each label in the dataset\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853082bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing a baseline with a DummyClassifier\n",
    "dummyc = DummyClassifier()\n",
    "dummyc.fit(X_train, y_train)\n",
    "\n",
    "# retrieving score for DummyClassifier\n",
    "score = dummyc.score(X_train, y_train)\n",
    "y_predicted = dummyc.predict(X_test)\n",
    "\n",
    "print(\"Score: \", score)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing data \n",
    "balancing = RandomUnderSampler(random_state=42)\n",
    "# balancing = SMOTE(random_state=42)\n",
    "# balancing = SMOTETomek(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = balancing.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ae644",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3da479",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0db0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing values\n",
    "missing = X_resampled.isnull()\n",
    "missing.sum()\n",
    "\n",
    "total_missing = X_resampled.isnull().sum()\n",
    "percent_missing = total_missing * 100 / len(X_resampled)\n",
    "missing_value_df = pd.DataFrame({'percent': percent_missing, 'total':total_missing})\n",
    "\n",
    "missing_value_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the balanced datasets for training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# calculating the percentage of each label in the dataset after undersampling\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934c78",
   "metadata": {},
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled_data = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled_data, columns=X_train.columns)\n",
    "\n",
    "X_train_scaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising test features\n",
    "X_test_scaled_data = scaler.fit_transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled_data, columns=X_test.columns)\n",
    "X_test_scaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2981bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Scaling\n",
    "\n",
    "X_train_scaled.plot(kind='box', figsize=(8, 5))\n",
    "plt.title('Boxplot of Features After Scaling')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Value')\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75842bc1",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising PCA with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# fitting PCA with features after undersampling and normalisation\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# reducing dimensionality in undersampled, normalised training dataset\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "print(\"Original shape: {}\".format(str(X_train_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_train_pca.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving variance captured by the components\n",
    "variance_captured = pca.explained_variance_ratio_\n",
    "print(variance_captured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_eigenvalues = np.cumsum(variance_captured)\n",
    "\n",
    "plt.bar(range(0,len(variance_captured)), variance_captured, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(X_train_pca, x=X_train_pca[:,0], y=X_train_pca[:,1], z=X_train_pca[:,2],\n",
    "              color=y_train)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a matrix representing the directions of maximum variance in the data\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1, 2], [\"1st component\", \"2nd component\", \"3rd component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(list(X_train_scaled.columns))), list(X_train_scaled.columns), rotation=60, ha='left')\n",
    "plt.title('Figure 9 - Features x PCA Transformation')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb00692",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising PCA for test data, with 2 components\n",
    "pca_test = PCA(n_components=3)\n",
    "\n",
    "# fitting PCA with test features after normalisation\n",
    "pca_test.fit(X_test_scaled)\n",
    "\n",
    "# reducing dimensionality in normalised test dataset\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"Original shape: {}\".format(str(X_test_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_test_pca.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyClassifier x Normalised data\n",
    "dummyc = DummyClassifier()\n",
    "dummyc.fit(X_train_pca, y_train)\n",
    "\n",
    "# confirming if score for Dummy classifier results from a balanced dataset\n",
    "score = dummyc.score(X_train_pca, y_train)\n",
    "y_predicted = dummyc.predict(X_test_pca)\n",
    "\n",
    "print(\"Score: \", score)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995e1c9",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce2618",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Start time:\", time.ctime(start_time))\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Dict containing different parameters for LinearSVC \n",
    "param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty':['l1', 'l2'],\n",
    "    'dual': [False, True]\n",
    "}\n",
    "\n",
    "# LinearSVC classifier\n",
    "lin_clf = LinearSVC()\n",
    "\n",
    "# GridSearchCV object is instantiated with these parameters and fit to training data\n",
    "grid_search_lsvc = GridSearchCV(estimator=lin_clf, param_grid=param_grid, verbose=1)\n",
    "\n",
    "# GridSearchCV object will be used to find the optimal combination of these parameters for the classification model\n",
    "grid_search_lsvc.fit(X_train_pca, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"End time:\", time.ctime(end_time))\n",
    "os.system('say \"Victor, the Hyperparameters for Linear SVC were successfully found.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5769c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best accuracy achieved in the grid search and the best estimator\n",
    "print(\"best accuracy\", grid_search_lsvc.best_score_)\n",
    "print(grid_search_lsvc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving optimal values for the parameters dual, loss, multi_class, and penalty from grid search\n",
    "optimal_dual = grid_search_lsvc.best_params_['dual']\n",
    "# optimal_loss = grid_search_lsvc.best_params_['loss']\n",
    "optimal_c = grid_search_lsvc.best_params_['C']\n",
    "optimal_penalty = grid_search_lsvc.best_params_['penalty']\n",
    "\n",
    "# instantiating a new LinearSVC classifier with optimal values\n",
    "lin_clf = LinearSVC(dual=optimal_dual, C=optimal_c, penalty=optimal_penalty)\n",
    "lin_clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# calculating score and accuracy\n",
    "score = lin_clf.score(X_train_pca, y_train)\n",
    "y_predicted_lin_clf = lin_clf.predict(X_test_pca)\n",
    "\n",
    "print(\"Score: \", score)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_predicted_lin_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7ddc7",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb07080",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50779d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Start time:\", time.ctime(start_time))\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# defining parameter grid for grid search\n",
    "param_grid = {\n",
    "    'max_iter': [1, 2, 5, 10, 20, 50, 100, 200, 500],\n",
    "    'loss': [\n",
    "        'hinge','log_loss','log','modified_huber','squared_hinge',\n",
    "        'perceptron','squared_error','huber',\n",
    "        'epsilon_insensitive','squared_epsilon_insensitive'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# instantiating SGD classifier\n",
    "SGD = SGDClassifier()\n",
    "\n",
    "# setting up grid search with SGD classifier and param_grid\n",
    "grid_search_sgd = GridSearchCV(estimator=SGD, param_grid=param_grid, verbose=1)\n",
    "\n",
    "# fitting grid search to training data\n",
    "grid_search_sgd.fit(X_train_pca, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"End time:\", time.ctime(end_time))\n",
    "os.system('say \"Victor, the Hyperparameters for SGD classifier were successfully found.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595966fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best accuracy achieved in the grid search and the best estimator.\n",
    "print(\"best accuracy\", grid_search_sgd.best_score_)\n",
    "print(grid_search_sgd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_max_iter = grid_search_sgd.best_params_['max_iter']\n",
    "optimal_loss = grid_search_sgd.best_params_['loss']\n",
    "# optimal_p = grid_search_knn.best_params_['p']\n",
    "\n",
    "# a new KNeighborsClassifier object created and fitted with the training data\n",
    "SGD = SGDClassifier(max_iter=optimal_max_iter, loss=optimal_loss)\n",
    "SGD.fit(X_train_pca, y_train)\n",
    "\n",
    "# calculating score and accuracy\n",
    "score = SGD.score(X_train_pca, y_train)\n",
    "y_predicted_sgd = SGD.predict(X_test_pca)\n",
    "\n",
    "print(\"Score: \", score)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae3bef",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e18eb",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Start time:\", time.ctime(start_time))\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# defining parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_neighbors': range(3, 218, 5),\n",
    "    'weights': ['uniform','distance']\n",
    "#     'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# instantiating kNN classifier\n",
    "kNN = KNeighborsClassifier()\n",
    "\n",
    "# setting up grid search with kNN classifier and param_grid\n",
    "grid_search_knn = GridSearchCV(estimator=kNN, param_grid=param_grid, verbose=1)\n",
    "\n",
    "# fitting grid search to training data\n",
    "grid_search_knn.fit(X_train_pca, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"End time:\", time.ctime(end_time))\n",
    "\n",
    "os.system('say \"Victor, the Hyperparameters for kNN were successfully found.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14799882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best accuracy achieved in the grid search and the best estimator.\n",
    "print(\"best accuracy\", grid_search_knn.best_score_)\n",
    "print(grid_search_knn.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24679603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling after optimal k (\"sweet spot\") and weights values were determined\n",
    "optimal_k = grid_search_knn.best_params_['n_neighbors']\n",
    "optimal_weight = grid_search_knn.best_params_['weights']\n",
    "# optimal_p = grid_search_knn.best_params_['p']\n",
    "\n",
    "# a new KNeighborsClassifier object created and fitted with the training data\n",
    "kNN = KNeighborsClassifier(n_neighbors=optimal_k, weights=optimal_weight)\n",
    "kNN.fit(X_train_pca, y_train)\n",
    "\n",
    "# calculating score and accuracy\n",
    "score = kNN.score(X_train_pca, y_train)\n",
    "y_predicted_knn = kNN.predict(X_test_pca)\n",
    "\n",
    "print(\"Score: \", score)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_predicted_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec933ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Start time:\", time.ctime(start_time))\n",
    "\n",
    "# initialising arrays for storing train and test accuracy\n",
    "neighbors = np.arange(3, 218, 5)     \n",
    "train_accuracy = np.zeros(len(neighbors))    \n",
    "test_accuracy = np.zeros(len(neighbors))    \n",
    "\n",
    "# looping over different values of k\n",
    "for i, k in enumerate(neighbors):                         \n",
    "    kNN = KNeighborsClassifier(n_neighbors=k, weights=optimal_weight)           \n",
    "    kNN.fit(X_train_pca, y_train)                             \n",
    "    train_accuracy[i] = kNN.score(X_train_pca, y_train)       \n",
    "    test_accuracy[i] = kNN.score(X_test_pca, y_test)  \n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"End time:\", time.ctime(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67729009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating figure, adding title\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.title('Figure 10 - kNN accuracy with varying number of neighbors', fontsize = 10)\n",
    "\n",
    "# plotting the test accuracy and traning accuracy x number of neighbours\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training accuracy')\n",
    "\n",
    "# adding legend, axes labels, setting font size and axes ticks\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.xlabel('Number of neighbors', fontsize = 10)\n",
    "plt.ylabel('Accuracy', fontsize = 10)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714cdd3",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d46df3",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a classification report for LinearSVC\n",
    "cr_linearsvc = metrics.classification_report(y_test, y_predicted_lin_clf, digits=4)\n",
    "\n",
    "# generating a classification report for SGD\n",
    "cr_sgd = metrics.classification_report(y_test, y_predicted_sgd, digits=4)\n",
    "\n",
    "# generating a classification report for kNN\n",
    "cr_knn = metrics.classification_report(y_test, y_predicted_knn, digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2ea3f",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing classification report for LinearSVC\n",
    "print(cr_linearsvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678b7e9",
   "metadata": {},
   "source": [
    "#### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing classification report for SGD\n",
    "print(cr_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba199b37",
   "metadata": {},
   "source": [
    "#### k-Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing classification report for kNN classifier\n",
    "print(cr_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240900a8",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0af6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting labels valid for all following confusion matrices\n",
    "cm_labels = ['No Sev Slug (0)', 'Sev Slug (1)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f407c6",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60319999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating confusion matrix using the test labels and the predicted labels from LinearSVC classifier\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted_lin_clf)\n",
    "\n",
    "# converting the confusion matrix into a pandas dataframe\n",
    "cm_dataframe = pd.DataFrame(cm, index=cm_labels, columns=cm_labels)\n",
    "\n",
    "# creating a heatmap using using seaborn\n",
    "sns.heatmap(cm_dataframe, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e2a36",
   "metadata": {},
   "source": [
    "#### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating confusion matrix using the test labels and the predicted labels from LinearSVC classifier\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted_sgd)\n",
    "\n",
    "# converting the confusion matrix into a pandas dataframe\n",
    "cm_dataframe = pd.DataFrame(cm, index=cm_labels, columns=cm_labels)\n",
    "\n",
    "# creating a heatmap using using seaborn\n",
    "sns.heatmap(cm_dataframe, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bbadb",
   "metadata": {},
   "source": [
    "#### k-Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating confusion matrix using the test labels and the predicted labels from kNeighbour Classifier\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted_knn)\n",
    "\n",
    "# converting the confusion matrix into a pandas dataframe\n",
    "cm_dataframe = pd.DataFrame(cm, index=cm_labels, columns=cm_labels)\n",
    "\n",
    "# creating a heatmap using using seaborn\n",
    "sns.heatmap(cm_dataframe, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44efe6e",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a 10-fold cross-validation for LinearSVC\n",
    "scores = cross_val_score(lin_clf, X_train_pca, y_train, cv=10)\n",
    "\n",
    "print(f\"Mean score: {np.mean(scores):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a923bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a 10-fold cross-validation for SGD\n",
    "scores = cross_val_score(SGD, X_train_pca, y_train, cv=10)\n",
    "\n",
    "print(f\"Mean score: {np.mean(scores):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5283508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a 10-fold cross validation for kNN\n",
    "scores = cross_val_score(kNN, X_train_pca, y_train, cv=10)\n",
    "\n",
    "print(f\"Mean score: {np.mean(scores):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d48761",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "* remove outliers?\n",
    "* other under sampling algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f1e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
